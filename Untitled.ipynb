{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5782755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans  # For unsupervised task\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1eef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset for all 70 patients\n",
    "data = []\n",
    "for i in range(1, 71):\n",
    "    filename = f\"data-{i:02d}\"\n",
    "    data.append(pd.read_csv(filename, sep=\"\\t\", header=None, usecols=[2, 3]))\n",
    "data = pd.concat(data)  # Combine into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb3c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for the third column\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')  # Handle potential unseen values\n",
    "encoded_data = encoder.fit_transform(data.iloc[:, 0].values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6848fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29330 entries, 0 to 340\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   2       29330 non-null  int64 \n",
      " 1   3       29297 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 687.4+ KB\n",
      "None\n",
      "                  2\n",
      "count  29330.000000\n",
      "mean      46.428606\n",
      "std       13.453219\n",
      "min        0.000000\n",
      "25%       33.000000\n",
      "50%       48.000000\n",
      "75%       60.000000\n",
      "max       72.000000\n",
      "2\n",
      "33    9518\n",
      "34    3830\n",
      "58    3518\n",
      "62    3160\n",
      "60    2771\n",
      "48    1883\n",
      "35    1053\n",
      "57     990\n",
      "64     904\n",
      "65     331\n",
      "67     326\n",
      "63     219\n",
      "66     154\n",
      "70     139\n",
      "56     119\n",
      "71      98\n",
      "72      94\n",
      "69      68\n",
      "61      66\n",
      "68      34\n",
      "0       33\n",
      "59      20\n",
      "4        1\n",
      "36       1\n",
      "Name: count, dtype: int64\n",
      "3\n",
      "6        1522\n",
      "0        1223\n",
      "3        1183\n",
      "2        1049\n",
      "4         951\n",
      "         ... \n",
      "327         1\n",
      "364         1\n",
      "258         1\n",
      "276         1\n",
      "378.0       1\n",
      "Name: count, Length: 744, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "print(data.info())  # Check data types and missing values\n",
    "print(data.describe())  # Summary statistics\n",
    "print(data.iloc[:, 0].value_counts())  # Frequency of unique features (column 3)\n",
    "print(data.iloc[:, 1].value_counts())  # Frequency of unique labels (column 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ddee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting function\n",
    "def split_data(data, train_size=0.6, val_size=0.1, test_size=0.3, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(data))\n",
    "    train_idx = indices[:int(train_size * len(data))]\n",
    "    val_idx = indices[int(train_size * len(data)):int((train_size + val_size) * len(data))]\n",
    "    test_idx = indices[int((train_size + val_size) * len(data)):]\n",
    "    train_data = data.iloc[train_idx].reset_index(drop=True)\n",
    "    val_data = data.iloc[val_idx].reset_index(drop=True)\n",
    "    test_data = data.iloc[test_idx].reset_index(drop=True)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b0afcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN implementation from scratch\n",
    "def knn(train_data, test_point, k):\n",
    "    distances = []\n",
    "    for train_point in train_data:\n",
    "        # Calculate Euclidean distance between test_point and train_point\n",
    "        distance = np.sqrt(np.sum((test_point - train_point) ** 2))\n",
    "        distances.append((train_point, distance))\n",
    "    distances.sort(key=lambda x: x[0])  # Sort by distance\n",
    "    nearest_neighbors = distances[:k]  # Select k nearest neighbors\n",
    "    # Get most frequent label among nearest neighbors\n",
    "    neighbor_labels = [neighbor[1] for neighbor in nearest_neighbors]\n",
    "    predicted_label = max(set(neighbor_labels), key=neighbor_labels.count)\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbfcbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[56.0, 33.0, 65.0, 60.0, 56.0, 60.0, 56.0, 31.0, 31.0, 56.0, 31.0, 60.0, 31.0, 58.0, 31.0, 60.0, 60.0, 31.0, 33.0, 58.0, 31.0, 55.0, 60.0, 60.0, 31.0, 60.0, 32.0, 58.0, 58.0, 31.0, 31.0, 58.0, 62.0, 63.0, 31.0, 58.0, 55.0, 31.0, 31.0, 31.0, 31.0, 31.0, 58.0, 58.0, 60.0, 31.0, 31.0, 56.0, 31.0, 32.0, 64.0, 65.0, 58.0, 32.0, 65.0, 56.0, 32.0, 69.0, 60.0, 60.0, 56.0, 56.0, 63.0, 32.0, 62.0, 56.0, 31.0, 32.0, 31.0, 31.0, 31.0, 31.0, 32.0, 60.0, 31.0, 60.0, 62.0, 31.0, 31.0, 46.0, 60.0, 31.0, 31.0, 31.0, 60.0, 31.0, 46.0, 31.0, 58.0, 31.0, 55.0, 56.0, 31.0, 31.0, 31.0, 32.0, 31.0, 60.0, 58.0, 56.0, 46.0, 56.0, 56.0, 31.0, 63.0, 31.0, 58.0, 31.0, 31.0, 31.0, 62.0, 56.0, 62.0, 31.0, 31.0, 32.0, 61.0, 56.0, 46.0, 56.0, 56.0, 32.0, 46.0, 60.0, 56.0, 62.0, 69.0, 60.0, 32.0, 56.0, 32.0, 56.0, 31.0, 56.0, 56.0, 60.0, 58.0, 32.0, 46.0, 31.0, 46.0, 31.0, 31.0, 31.0, 60.0, 32.0, 58.0, 46.0, 58.0, 56.0, 31.0, 46.0, 60.0, 60.0, 32.0, 46.0, 58.0, 31.0, 60.0, 56.0, 56.0, 32.0, 31.0, 31.0, 31.0, 31.0, 58.0, 31.0, 56.0, 32.0, 31.0, 46.0, 31.0, 60.0, 55.0, 31.0, 32.0, 46.0, 32.0, 31.0, 31.0, 60.0, 56.0, 31.0, 58.0, 31.0, 31.0, 32.0, 32.0, 58.0, 64.0, 58.0, 56.0, 31.0, 33.0, 58.0, 32.0, 58.0, 31.0, 60.0, 58.0, 60.0, 55.0, 63.0, 46.0, 46.0, 63.0, 31.0, 46.0, 56.0, 56.0, 32.0, 66.0, 31.0, 60.0, 31.0, 31.0, 63.0, 60.0, 46.0, 61.0, 56.0, 55.0, 31.0, 46.0, 60.0, 31.0, 31.0, 31.0, 33.0, 31.0, 60.0, 56.0, 31.0, 32.0, 46.0, 31.0, 31.0, 31.0, 32.0, 31.0, 31.0, 31.0, 31.0, 56.0, 58.0, 58.0, 31.0, 31.0, 31.0, 31.0, 60.0, 60.0, 68.0, 65.0, 32.0, 31.0, 31.0, 31.0, 31.0, 63.0, 59.0, 56.0, 56.0, 31.0, 32.0, 46.0, 56.0, 31.0, 31.0, 58.0, 31.0, 31.0, 31.0, 31.0, 33.0, 31.0, 31.0, 32.0, 31.0, 31.0, 60.0, 31.0, 31.0, 58.0, 46.0, 31.0, 65.0, 31.0, 56.0, 62.0, 60.0, 31.0, 60.0, 55.0, 33.0, 55.0, 46.0, 31.0, 31.0, 33.0, 60.0, 46.0, 31.0, 55.0, 56.0, 31.0, 55.0, 60.0, 58.0, 31.0, 58.0, 56.0, 46.0, 32.0, 32.0, 46.0, 31.0, 56.0, 56.0, 31.0, 55.0, 31.0, 31.0, 31.0, 31.0, 60.0, 56.0, 46.0, 32.0, 31.0, 56.0, 32.0, 31.0, 56.0, 60.0, 31.0, 58.0, 60.0, 33.0, 60.0, 60.0, 32.0, 60.0, 58.0, 31.0, 31.0, 31.0, 32.0, 31.0, 60.0, 58.0, 58.0, 31.0, 58.0, 31.0, 32.0, 32.0, 31.0, 31.0, 58.0, 32.0, 46.0, 31.0, 58.0, 56.0, 31.0, 54.0, 31.0, 58.0, 31.0, 60.0, 56.0, 56.0, 46.0, 56.0, 55.0, 60.0, 31.0, 31.0, 31.0, 32.0, 46.0, 62.0, 46.0, 62.0, 31.0, 62.0, 31.0, 31.0, 56.0, 32.0, 58.0, 32.0, 31.0, 58.0, 60.0, 60.0, 67.0, 33.0, 56.0, 55.0, 32.0, 32.0, 60.0, 58.0, 31.0, 32.0, 56.0, 58.0, 56.0, 56.0, 31.0, 32.0, 60.0, 56.0, 31.0, 31.0, 60.0, 58.0, 60.0, 31.0, 56.0, 31.0, 60.0, 58.0, 32.0, 31.0, 32.0, 58.0, 32.0, 46.0, 58.0, 63.0, 56.0, 31.0, 60.0, 60.0, 31.0, 31.0, 32.0, 31.0, 31.0, 32.0, 56.0, 62.0, 31.0, 58.0, 56.0, 55.0, 32.0, 60.0, 33.0, 56.0, 32.0, 32.0, 31.0, 62.0, 31.0, 46.0, 60.0, 32.0, 32.0, 31.0, 60.0, 31.0, 32.0, 60.0, 31.0, 56.0, 32.0, 56.0, 60.0, 31.0, 33.0, 32.0, 32.0, 60.0, 62.0, 56.0, 31.0, 58.0, 31.0, 56.0, 31.0, 32.0, 31.0, 32.0, 56.0, 65.0, 31.0, 67.0, 58.0, 31.0, 58.0, 56.0, 46.0, 31.0, 46.0, 33.0, 58.0, 32.0, 31.0, 31.0, 60.0, 58.0, 31.0, 31.0, 58.0, 60.0, 46.0, 60.0, 31.0, 31.0, 31.0, 46.0, 58.0, 31.0, 31.0, 31.0, 55.0, 31.0, 32.0, 57.0, 62.0, 60.0, 60.0, 58.0, 31.0, 46.0, 31.0, 62.0, 32.0, 58.0, 56.0, 33.0, 60.0, 60.0, 32.0, 58.0, 46.0, 32.0, 32.0, 58.0, 58.0, 60.0, 55.0, 31.0, 61.0, 56.0, 46.0, 31.0, 56.0, 60.0, 60.0, 31.0, 58.0, 32.0, 55.0, 32.0, 32.0, 58.0, 55.0, 58.0, 56.0, 31.0, 60.0, 58.0, 56.0, 31.0, 56.0, 60.0, 32.0, 58.0, 58.0, 31.0, 56.0, 63.0, 32.0, 31.0, 55.0, 62.0, 56.0, 58.0, 62.0, 31.0, 31.0, 31.0, 31.0, 31.0, 55.0, 58.0, 31.0, 46.0, 33.0, 33.0, 64.0, 56.0, 55.0, 31.0, 56.0, 31.0, 31.0, 56.0, 33.0, 58.0, 60.0, 31.0, 60.0, 58.0, 58.0, 56.0, 33.0, 56.0, 32.0, 31.0, 56.0, 31.0, 32.0, 60.0, 32.0, 31.0, 31.0, 31.0, 31.0, 56.0, 60.0, 61.0, 65.0, 58.0, 31.0, 62.0, 31.0, 32.0, 60.0, 46.0, 32.0, 31.0, 31.0, 65.0, 60.0, 33.0, 62.0, 46.0, 31.0, 31.0, 31.0, 64.0, 58.0, 32.0, 31.0, 31.0, 31.0, 32.0, 60.0, 31.0, 32.0, 56.0, 32.0, 56.0, 58.0, 32.0, 32.0, 70.0, 46.0, 46.0, 60.0, 31.0, 31.0, 31.0, 31.0, 31.0, 32.0, 69.0, 56.0, 31.0, 46.0, 32.0, 56.0, 60.0, 31.0, 64.0, 31.0, 60.0, 59.0, 56.0, 31.0, 56.0, 59.0, 46.0, 46.0, 32.0, 56.0, 55.0, 32.0, 60.0, 32.0, 32.0, 31.0, 31.0, 55.0, 31.0, 56.0, 58.0, 60.0, 58.0, 32.0, 55.0, 31.0, 60.0, 56.0, 31.0, 56.0, 46.0, 60.0, 31.0, 60.0, 58.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 33.0, 33.0, 46.0, 58.0, 32.0, 32.0, 31.0, 31.0, 55.0, 58.0, 31.0, 55.0, 31.0, 58.0, 31.0, 64.0, 60.0, 58.0, 32.0, 32.0, 32.0, 46.0, 32.0, 31.0, 32.0, 31.0, 31.0, 32.0, 56.0, 31.0, 46.0, 55.0, 55.0, 60.0, 31.0, 55.0, 56.0, 60.0, 62.0, 55.0, 32.0, 31.0, 31.0, 31.0, 58.0, 31.0, 60.0, 31.0, 32.0, 58.0, 61.0, 31.0, 58.0, 33.0, 60.0, 46.0, 32.0, 58.0, 31.0, 58.0, 55.0, 46.0, 32.0, 31.0, 32.0, 58.0, 31.0, 31.0, 31.0, 46.0, 58.0, 31.0, 33.0, 58.0, 56.0, 60.0, 64.0, 60.0, 55.0, 31.0, 60.0, 31.0, 60.0, 60.0, 56.0, 31.0, 32.0, 31.0, 32.0, 32.0, 63.0, 56.0, 56.0, 60.0, 60.0, 56.0, 46.0, 31.0, 46.0, 60.0, 58.0, 31.0, 60.0, 32.0, 32.0, 31.0, 58.0, 58.0, 60.0, 32.0, 62.0, 56.0, 46.0, 60.0, 62.0, 62.0, 56.0, 60.0, 31.0, 60.0, 55.0, 62.0, 31.0, 46.0, 46.0, 60.0, 56.0, 58.0, 31.0, 46.0, 56.0, 56.0, 31.0, 60.0, 32.0, 46.0, 55.0, 58.0, 32.0, 31.0, 31.0, 46.0, 31.0, 33.0, 31.0, 56.0, 46.0, 32.0, 31.0, 58.0, 32.0, 31.0, 54.0, 32.0, 32.0, 31.0, 55.0, 56.0, 46.0, 31.0, 31.0, 46.0, 56.0, 31.0, 31.0, 31.0, 31.0, 32.0, 69.0, 31.0, 56.0, 56.0, 32.0, 31.0, 31.0, 32.0, 31.0, 32.0, 64.0, 31.0, 46.0, 32.0, 31.0, 60.0, 56.0, 58.0, 56.0, 60.0, 31.0, 31.0, 32.0, 32.0, 55.0, 46.0, 31.0, 58.0, 60.0, 31.0, 31.0, 58.0, 46.0, 31.0, 46.0, 58.0, 60.0, 60.0, 56.0, 32.0, 32.0, 31.0, 31.0, 56.0, 32.0, 31.0, 60.0, 31.0, 31.0, 32.0, 56.0, 33.0, 31.0, 31.0, 31.0, 31.0, 31.0, 46.0, 31.0, 32.0, 46.0, 31.0, 31.0, 32.0, 33.0, 67.0, 56.0, 58.0, 31.0, 31.0, 56.0, 60.0, 60.0, 31.0, 56.0, 56.0, 46.0, 31.0, 58.0, 31.0, 64.0, 31.0, 58.0, 31.0, 31.0, 31.0, 33.0, 60.0, 63.0, 31.0, 32.0, 31.0, 31.0, 58.0, 31.0, 56.0, 32.0, 31.0, 56.0, 46.0, 60.0, 60.0, 56.0, 31.0, 68.0, 60.0, 56.0, 31.0, 31.0, 62.0, 31.0, 32.0, 31.0, 31.0, 56.0, 58.0, 31.0, 31.0, 31.0, 56.0, 31.0, 56.0, 32.0, 58.0, 56.0, 33.0, 31.0, 31.0, 31.0, 61.0, 32.0, 60.0, 56.0, 31.0, 56.0, 56.0, 32.0, 60.0, 31.0, 60.0, 56.0, 31.0, 32.0, 31.0, 32.0, 31.0, 60.0, 31.0, 31.0, 65.0, 31.0, 31.0, 31.0, 31.0, 31.0, 54.0, 31.0, 31.0, 31.0, 46.0, 55.0, 58.0, 60.0, 65.0, 32.0, 58.0, 31.0, 33.0, 46.0, 32.0, 58.0, 32.0, 31.0, 32.0, 62.0, 31.0, 31.0, 31.0, 31.0, 46.0, 60.0, 46.0, 31.0, 31.0, 31.0, 58.0, 46.0, 60.0, 31.0, 60.0, 31.0, 32.0, 31.0, 33.0, 56.0, 31.0, 31.0, 31.0, 31.0, 56.0, 31.0, 55.0, 31.0, 31.0, 31.0, 32.0, 32.0, 33.0, 62.0, 33.0, 58.0, 31.0, 32.0, 46.0, 67.0, 62.0, 33.0, 60.0, 60.0, 32.0, 31.0, 46.0, 70.0, 31.0, 32.0, 55.0, 32.0, 31.0, 31.0, 32.0, 31.0, 60.0, 32.0, 32.0, 31.0, 32.0, 46.0, 33.0, 31.0, 58.0, 32.0, 56.0, 32.0, 31.0, 31.0, 60.0, 33.0, 60.0, 31.0, 60.0, 31.0, 58.0, 56.0, 60.0, 46.0, 60.0, 31.0, 31.0, 58.0, 46.0, 31.0, 68.0, 60.0, 56.0, 58.0, 65.0, 31.0, 31.0, 31.0, 33.0, 31.0, 31.0, 32.0, 58.0, 70.0, 32.0, 46.0, 46.0, 33.0, 31.0, 31.0, 60.0, 31.0, 31.0, 31.0, 46.0, 31.0, 58.0, 31.0, 60.0, 32.0, 46.0, 46.0, 60.0, 56.0, 60.0, 56.0, 63.0, 56.0, 60.0, 33.0, 31.0, 32.0, 58.0, 31.0, 46.0, 56.0, 31.0, 31.0, 60.0, 58.0, 61.0, 56.0, 60.0, 58.0, 33.0, 60.0, 55.0, 56.0, 56.0, 31.0, 60.0, 46.0, 55.0, 32.0, 31.0, 31.0, 55.0, 61.0, 56.0, 31.0, 31.0, 31.0, 31.0, 31.0, 60.0, 58.0, 58.0, 62.0, 60.0, 31.0, 46.0, 31.0, 31.0, 33.0, 31.0, 58.0, 31.0, 46.0, 31.0, 60.0, 56.0, 32.0, 31.0, 70.0, 56.0, 55.0, 31.0, 58.0, 46.0, 31.0, 31.0, 58.0, 60.0, 61.0, 31.0, 31.0, 46.0, 32.0, 58.0, 32.0, 56.0, 32.0, 31.0, 31.0, 31.0, 56.0, 60.0, 56.0, 46.0, 56.0, 31.0, 62.0, 46.0, 31.0, 32.0, 60.0, 31.0, 67.0, 31.0, 31.0, 31.0, 32.0, 31.0, 31.0, 31.0, 31.0, 32.0, 55.0, 31.0, 32.0, 32.0, 31.0, 31.0, 58.0, 60.0, 65.0, 32.0, 62.0, 60.0, 31.0, 60.0, 56.0, 32.0, 60.0, 33.0, 62.0, 32.0, 58.0, 56.0, 58.0, 58.0, 60.0, 56.0, 55.0, 58.0, 62.0, 31.0, 32.0, 46.0, 31.0, 31.0, 31.0, 31.0, 46.0, 58.0, 33.0, 31.0, 56.0, 31.0, 32.0, 56.0, 32.0, 33.0, 58.0, 56.0, 58.0, 32.0, 60.0, 60.0, 33.0, 60.0, 60.0, 56.0, 60.0, 31.0, 32.0, 46.0, 31.0, 31.0, 58.0, 33.0, 58.0, 32.0, 60.0, 31.0, 31.0, 61.0, 56.0, 65.0, 31.0, 33.0, 58.0, 56.0, 58.0, 31.0, 55.0, 31.0, 32.0, 58.0, 33.0, 60.0, 60.0, 56.0, 58.0, 46.0, 32.0, 33.0, 46.0, 46.0, 46.0, 46.0, 33.0, 58.0, 46.0, 32.0, 60.0, 60.0, 46.0, 31.0, 31.0, 62.0, 31.0, 31.0, 46.0, 31.0, 32.0, 58.0, 60.0, 33.0, 58.0, 46.0, 31.0, 58.0, 31.0, 56.0, 62.0, 31.0, 62.0, 32.0, 56.0, 56.0, 56.0, 58.0, 60.0, 31.0, 31.0, 56.0, 31.0, 31.0, 31.0, 58.0, 62.0, 55.0, 31.0, 31.0, 33.0, 60.0, 32.0, 32.0, 58.0, 62.0, 31.0, 31.0, 55.0, 31.0, 56.0, 56.0, 56.0, 56.0, 60.0, 60.0, 32.0, 32.0, 56.0, 58.0, 56.0, 60.0, 33.0, 58.0, 56.0, 58.0, 31.0, 58.0, 58.0, 32.0, 31.0, 31.0, 46.0, 56.0, 58.0, 32.0, 46.0, 31.0, 46.0, 31.0, 32.0, 62.0, 31.0, 60.0, 31.0, 32.0, 60.0, 33.0, 65.0, 31.0, 31.0, 32.0, 31.0, 56.0, 56.0, 31.0, 60.0, 31.0, 60.0, 56.0, 31.0, 32.0, 61.0, 56.0, 31.0, 70.0, 31.0, 56.0, 32.0, 31.0, 55.0, 56.0, 31.0, 31.0, 56.0, 46.0, 56.0, 32.0, 58.0, 31.0, 31.0, 46.0, 46.0, 60.0, 56.0, 31.0, 60.0, 32.0, 60.0, 31.0, 33.0, 31.0, 46.0, 62.0, 55.0, 65.0, 60.0, 32.0, 58.0, 60.0, 32.0, 56.0, 33.0, 31.0, 58.0, 33.0, 32.0, 32.0, 31.0, 55.0, 31.0, 60.0, 60.0, 56.0, 56.0, 31.0, 31.0, 56.0, 31.0, 31.0, 46.0, 58.0, 32.0, 55.0, 58.0, 31.0, 32.0, 56.0, 31.0, 56.0, 31.0, 31.0, 67.0, 33.0, 58.0, 56.0, 46.0, 60.0, 56.0, 31.0, 32.0, 46.0, 32.0, 58.0, 32.0, 56.0, 46.0, 58.0, 31.0, 32.0, 56.0, 58.0, 33.0, 31.0, 33.0, 33.0, 33.0, 31.0, 60.0, 58.0, 56.0, 31.0, 56.0, 32.0, 31.0, 60.0, 31.0, 58.0, 60.0, 31.0, 60.0, 33.0, 31.0, 31.0, 31.0, 32.0, 32.0, 32.0, 54.0, 31.0, 31.0, 56.0, 58.0, 60.0, 58.0, 56.0, 31.0, 32.0, 32.0, 61.0, 64.0, 31.0, 31.0, 32.0, 58.0, 65.0, 31.0, 58.0, 32.0, 58.0, 56.0, 55.0, 58.0, 31.0, 31.0, 56.0, 32.0, 33.0, 31.0, 31.0, 31.0, 55.0, 32.0, 60.0, 31.0, 55.0, 31.0, 32.0, 60.0, 31.0, 32.0, 60.0, 31.0, 54.0, 60.0, 32.0, 31.0, 46.0, 60.0, 62.0, 31.0, 32.0, 31.0, 56.0, 31.0, 59.0, 31.0, 58.0, 56.0, 46.0, 31.0, 56.0, 31.0, 58.0, 31.0, 55.0, 46.0, 56.0, 31.0, 31.0, 56.0, 32.0, 58.0, 56.0, 60.0, 31.0, 56.0, 32.0, 31.0, 56.0, 32.0, 32.0, 46.0, 31.0, 31.0, 56.0, 32.0, 33.0, 31.0, 58.0, 32.0, 31.0, 56.0, 31.0, 31.0, 32.0, 58.0, 32.0, 32.0, 46.0, 32.0, 32.0, 31.0, 2.0, 31.0, 55.0, 58.0, 56.0, 62.0, 58.0, 31.0, 32.0, 60.0, 60.0, 46.0, 60.0, 58.0, 46.0, 31.0, 58.0, 32.0, 31.0, 33.0, 31.0, 31.0, 70.0, 58.0, 33.0, 60.0, 60.0, 46.0, 32.0, 55.0, 31.0, 58.0, 31.0, 33.0, 46.0, 56.0, 33.0, 58.0, 60.0, 32.0, 60.0, 58.0, 57.0, 62.0, 70.0, 32.0, 56.0, 31.0, 60.0, 61.0, 58.0, 46.0, 31.0, 46.0, 46.0, 32.0, 56.0, 31.0, 64.0, 32.0, 31.0, 60.0, 46.0, 58.0, 58.0, 58.0, 60.0, 46.0, 60.0, 31.0, 46.0, 65.0, 58.0, 31.0, 31.0, 31.0, 46.0, 61.0, 32.0, 31.0, 56.0, 31.0, 58.0, 31.0, 60.0, 46.0, 31.0, 32.0, 46.0, 31.0, 56.0, 60.0, 56.0, 31.0, 55.0, 31.0, 60.0, 58.0, 58.0, 31.0, 60.0, 31.0, 60.0, 31.0, 56.0, 60.0, 56.0, 32.0, 31.0, 31.0, 46.0, 56.0, 62.0, 46.0, 31.0, 55.0, 60.0, 58.0, 31.0, 64.0, 31.0, 56.0, 46.0, 31.0, 46.0, 56.0, 32.0, 31.0, 56.0, 32.0, 56.0, 46.0, 31.0, 31.0, 46.0, 32.0, 31.0, 32.0, 32.0, 31.0, 67.0, 58.0, 55.0, 31.0, 32.0, 32.0, 60.0, 56.0, 31.0, 31.0, 60.0, 32.0, 31.0, 46.0, 60.0, 56.0, 32.0, 33.0, 62.0, 60.0, 31.0, 60.0, 31.0, 31.0, 54.0, 31.0, 33.0, 31.0, 46.0, 32.0, 31.0, 46.0, 60.0, 62.0, 56.0, 60.0, 62.0, 31.0, 68.0, 31.0, 62.0, 46.0, 56.0, 32.0, 31.0, 31.0, 46.0, 31.0, 33.0, 31.0, 60.0, 58.0, 33.0, 33.0, 56.0, 58.0, 32.0, 55.0, 31.0, 32.0, 60.0, 62.0, 56.0, 60.0, 62.0, 32.0, 58.0, 62.0, 31.0, 60.0, 56.0, 58.0, 61.0, 31.0, 32.0, 60.0, 31.0, 56.0, 31.0, 32.0, 60.0, 65.0, 31.0, 31.0, 56.0, 31.0, 64.0, 31.0, 60.0, 31.0, 31.0, 58.0, 32.0, 46.0, 31.0, 31.0, 32.0, 55.0, 65.0, 58.0, 60.0, 56.0, 56.0, 31.0, 63.0, 33.0, 61.0, 55.0, 56.0, 31.0, 32.0, 58.0, 31.0, 46.0, 31.0, 32.0, 56.0, 32.0, 31.0, 33.0, 58.0, 60.0, 56.0, 31.0, 46.0, 60.0, 60.0, 56.0, 31.0, 56.0, 56.0, 32.0, 32.0, 56.0, 32.0, 32.0, 31.0, 31.0, 31.0, 32.0, 31.0, 56.0, 65.0, 56.0, 31.0, 32.0, 32.0, 31.0, 31.0, 58.0, 31.0, 58.0, 60.0, 31.0, 31.0, 31.0, 55.0, 55.0, 32.0, 60.0, 31.0, 32.0, 58.0, 60.0, 31.0, 55.0, 31.0, 56.0, 31.0, 31.0, 31.0, 32.0, 62.0, 56.0, 32.0, 31.0, 58.0, 58.0, 56.0, 61.0, 61.0, 62.0, 62.0, 46.0, 31.0, 31.0, 60.0, 31.0, 31.0, 46.0, 60.0, 33.0, 31.0, 31.0, 31.0, 56.0, 31.0, 60.0, 31.0, 65.0, 46.0, 62.0, 32.0, 55.0, 31.0, 56.0, 64.0, 31.0, 60.0, 31.0, 60.0, 31.0, 60.0, 31.0, 46.0, 31.0, 46.0, 31.0, 56.0, 60.0, 33.0, 58.0, 57.0, 58.0, 60.0, 31.0, 58.0, 56.0, 32.0, 62.0, 31.0, 32.0, 46.0, 31.0, 31.0, 32.0, 68.0, 31.0, 58.0, 32.0, 31.0, 31.0, 33.0, 56.0, 31.0, 31.0, 58.0, 2.0, 60.0, 58.0, 46.0, 32.0, 58.0, 31.0, 58.0, 32.0, 58.0, 31.0, 60.0, 31.0, 60.0, 63.0, 62.0, 58.0, 31.0, 31.0, 60.0, 55.0, 56.0, 31.0, 32.0, 55.0, 58.0, 46.0, 60.0, 31.0, 32.0, 65.0, 31.0, 31.0, 60.0, 32.0, 56.0, 60.0, 31.0, 31.0, 58.0, 46.0, 31.0, 56.0, 60.0, 58.0, 31.0, 62.0, 31.0, 46.0, 60.0, 56.0, 31.0, 32.0, 58.0, 33.0, 56.0, 31.0, 56.0, 70.0, 56.0, 55.0, 60.0, 60.0, 31.0, 32.0, 58.0, 60.0, 32.0, 46.0, 58.0, 32.0, 32.0, 60.0, 31.0, 56.0, 31.0, 32.0, 31.0, 63.0, 46.0, 56.0, 32.0, 31.0, 31.0, 31.0, 46.0, 56.0, 46.0, 62.0, 61.0, 55.0, 58.0, 46.0, 56.0, 46.0, 60.0, 62.0, 46.0, 31.0, 62.0, 31.0, 32.0, 68.0, 60.0, 31.0, 56.0, 31.0, 55.0, 31.0, 32.0, 62.0, 60.0, 60.0, 46.0, 31.0, 56.0, 31.0, 33.0, 56.0, 46.0, 58.0, 31.0, 70.0, 60.0, 60.0, 32.0, 31.0, 46.0, 58.0, 60.0, 31.0, 31.0, 60.0, 31.0, 31.0, 31.0, 56.0, 56.0, 32.0, 31.0, 31.0, 58.0, 54.0, 31.0, 31.0, 31.0, 32.0, 62.0, 33.0, 31.0, 31.0, 31.0, 31.0, 56.0, 56.0, 31.0, 31.0, 31.0, 31.0, 31.0, 60.0, 31.0, 46.0, 58.0, 58.0, 31.0, 32.0, 31.0, 58.0, 31.0, 56.0, 46.0, 56.0, 32.0, 55.0, 65.0, 60.0, 56.0, 32.0, 31.0, 31.0, 31.0, 60.0, 70.0, 32.0, 58.0, 56.0, 58.0, 32.0, 32.0, 32.0, 31.0, 60.0, 58.0, 58.0, 31.0, 32.0, 31.0, 32.0, 46.0, 31.0, 56.0, 31.0, 60.0, 31.0, 55.0, 56.0, 31.0, 65.0, 58.0, 58.0, 31.0, 31.0, 31.0, 58.0, 31.0, 61.0, 56.0, 32.0, 58.0, 33.0, 60.0, 46.0, 31.0, 31.0, 65.0, 56.0, 56.0, 32.0, 32.0, 33.0, 31.0, 60.0, 60.0, 32.0, 60.0, 56.0, 46.0, 60.0, 31.0, 60.0, 31.0, 33.0, 31.0, 31.0, 33.0, 31.0, 60.0, 31.0, 58.0, 54.0, 65.0, 31.0, 62.0, 60.0, 58.0, 46.0, 56.0, 31.0, 60.0, 63.0, 65.0, 32.0, 32.0, 31.0, 31.0, 32.0, 31.0, 31.0, 60.0, 58.0, 46.0, 31.0, 31.0, 33.0, 62.0, 31.0, 31.0, 31.0, 70.0, 31.0, 31.0, 60.0, 63.0, 31.0, 58.0, 31.0, 65.0, 32.0, 32.0, 31.0, 60.0, 46.0, 31.0, 31.0, 60.0, 31.0, 32.0, 32.0, 62.0, 60.0, 31.0, 58.0, 31.0, 56.0, 32.0, 60.0, 56.0, 60.0, 31.0, 58.0, 70.0, 31.0, 62.0, 31.0, 31.0, 31.0, 32.0, 56.0, 56.0, 31.0, 31.0, 46.0, 31.0, 60.0, 31.0, 32.0, 32.0, 32.0, 58.0, 32.0, 70.0, 32.0, 31.0, 31.0, 32.0, 55.0, 32.0, 54.0, 32.0, 32.0, 33.0, 60.0, 32.0, 31.0, 31.0, 60.0, 60.0, 46.0, 31.0, 46.0, 32.0, 31.0, 31.0, 46.0, 32.0, 62.0, 60.0, 56.0, 60.0, 58.0, 31.0, 58.0, 2.0, 46.0, 60.0, 63.0, 61.0, 56.0, 56.0, 62.0, 32.0, 58.0, 31.0, 55.0, 31.0, 46.0, 31.0, 32.0, 62.0, 58.0, 60.0, 60.0, 31.0, 32.0, 31.0, 31.0, 31.0, 60.0, 56.0, 58.0, 56.0, 33.0, 58.0, 32.0, 31.0, 31.0, 56.0, 56.0, 31.0, 60.0, 31.0, 31.0, 65.0, 31.0, 55.0, 58.0, 46.0, 60.0, 32.0, 56.0, 46.0, 31.0, 32.0, 56.0, 60.0, 31.0, 60.0, 55.0, 66.0, 46.0, 31.0, 56.0, 63.0, 60.0, 65.0, 32.0, 31.0, 60.0, 32.0, 55.0, 56.0, 58.0, 56.0, 58.0, 56.0, 56.0, 31.0, 62.0, 46.0, 31.0, 56.0, 32.0, 32.0, 31.0, 58.0, 31.0, 60.0, 31.0, 56.0, 60.0, 46.0, 31.0, 31.0, 58.0, 68.0, 58.0, 63.0, 31.0, 60.0, 56.0, 31.0, 60.0, 63.0, 32.0, 32.0, 60.0, 60.0, 58.0, 31.0, 31.0, 56.0, 46.0, 31.0, 46.0, 56.0, 31.0, 33.0, 58.0, 31.0, 32.0, 31.0, 31.0, 31.0, 58.0, 31.0, 58.0, 33.0, 32.0, 31.0, 62.0, 31.0, 63.0, 60.0, 31.0, 33.0, 56.0, 31.0, 56.0, 58.0, 31.0, 56.0, 31.0, 31.0, 60.0, 31.0, 31.0, 32.0, 58.0, 32.0, 56.0, 31.0, 56.0, 58.0, 31.0, 56.0, 33.0, 31.0, 31.0, 31.0, 46.0, 46.0, 60.0, 31.0, 31.0, 33.0, 33.0, 46.0, 31.0, 56.0, 60.0, 55.0, 65.0, 33.0, 32.0, 56.0, 56.0, 32.0, 31.0, 31.0, 62.0, 31.0, 60.0, 60.0, 58.0, 31.0, 60.0, 31.0, 31.0, 58.0, 31.0, 32.0, 31.0, 58.0, 58.0, 31.0, 31.0, 31.0, 32.0, 58.0, 31.0, 31.0, 32.0, 58.0, 31.0, 32.0, 59.0, 31.0, 31.0, 62.0, 31.0, 60.0, 31.0, 31.0, 32.0, 31.0, 60.0, 56.0, 56.0, 58.0, 46.0, 32.0, 31.0, 31.0, 58.0, 58.0, 31.0, 31.0, 32.0, 55.0, 60.0, 46.0, 33.0, 60.0, 55.0, 31.0, 60.0, 56.0, 31.0, 60.0, 55.0, 32.0, 31.0, 32.0, 58.0, 31.0, 31.0, 31.0, 62.0, 31.0, 32.0, 31.0, 31.0, 31.0, 32.0, 31.0, 56.0, 56.0, 31.0, 31.0, 33.0, 46.0, 60.0, 32.0, 60.0, 59.0, 55.0, 31.0, 58.0, 56.0, 32.0, 56.0, 46.0, 60.0, 33.0, 60.0, 31.0, 31.0, 31.0, 31.0, 62.0, 62.0, 31.0, 32.0, 56.0, 33.0, 60.0, 31.0, 58.0, 31.0, 56.0, 31.0, 31.0, 58.0, 58.0, 31.0, 46.0, 31.0, 31.0, 31.0, 60.0, 31.0, 46.0, 32.0, 33.0, 62.0, 56.0, 46.0, 55.0, 31.0, 31.0, 56.0, 62.0, 32.0, 46.0, 31.0, 31.0, 56.0, 59.0, 33.0, 32.0, 46.0, 31.0, 58.0, 60.0, 32.0, 31.0, 60.0, 31.0, 61.0, 61.0, 56.0, 62.0, 31.0, 62.0, 31.0, 31.0, 56.0, 58.0, 31.0, 32.0, 58.0, 32.0, 56.0, 63.0, 32.0, 56.0, 31.0, 58.0, 31.0, 31.0, 60.0, 31.0, 56.0, 56.0, 32.0, 63.0, 31.0, 60.0, 31.0, 46.0, 55.0, 62.0, 31.0, 56.0, 32.0, 31.0, 46.0, 32.0, 31.0, 60.0, 56.0, 67.0, 55.0, 31.0, 31.0, 56.0, 32.0, 60.0, 65.0, 56.0, 60.0, 33.0, 60.0, 60.0, 31.0, 32.0, 60.0, 33.0, 32.0, 55.0, 60.0, 62.0, 60.0, 60.0, 60.0, 31.0, 56.0, 31.0, 56.0, 56.0, 60.0, 31.0, 58.0, 31.0, 56.0, 33.0, 33.0, 70.0, 31.0, 2.0, 56.0, 62.0, 55.0, 58.0, 58.0, 31.0, 58.0, 32.0, 32.0, 55.0, 58.0, 31.0, 55.0, 31.0, 58.0, 31.0, 60.0, 31.0, 58.0, 31.0, 60.0, 31.0, 32.0, 31.0, 62.0, 31.0, 60.0, 31.0, 60.0, 62.0, 60.0, 31.0, 31.0, 46.0, 32.0, 32.0, 56.0, 32.0, 32.0, 31.0, 62.0, 33.0, 31.0, 60.0, 56.0, 32.0, 60.0, 61.0, 32.0, 33.0, 31.0, 31.0, 56.0, 31.0, 60.0, 31.0, 58.0, 56.0, 31.0, 31.0, 64.0, 62.0, 33.0]\n",
      "2933\n",
      "2933\n",
      "4\n",
      "0\n",
      "[157 14 0 76 129 249 144 4 233 3 153 13 94 116 101 9 12 58 6 57 158 127\n",
      " 228 '189' '197' 207 252 245 261 8 10 '006' 399 50 206 1 '043' 5 33 81 7.0\n",
      " 232 46 122 79 211 27 220 '211' 16 15 2 113 87 356.0 178 159 139 112 104\n",
      " '004' 24 288 74 163 263 188 202 210 84 156 96 267 34 119 60 213 165 256\n",
      " 170 105 154 254 56 160 20 64.0 '130' 195 18 59 152 '306' 78 166 140 360\n",
      " 227 55 295 223 '184' '086' 297 '018' 141 147 131 '195' 11 143 204 142 120\n",
      " 246 317.0 171 110 501 92 161 '134' 115 17 '210' 40 '204' 345 381 272 347\n",
      " 109 75 136 '010' 98 '070' 290 99 48 '083' 30 155 '128' 80 54 186 183\n",
      " 145.0 44 100 118 176 28 90 292 221 45 137 236 201 95 62 43 306 301 278\n",
      " 258 19 '155' 106 61 174 240 226 203 97 25 225 '135' '016' 133 88 '139' 35\n",
      " 70 164 111 82 125 234 279 31 326 77 184 3.5 384 185 349 148 336 85 '092'\n",
      " 67 126 107 243 '174' 242 319 134 83 364 '048' 192 '060' 259 '196' 270 333\n",
      " 298 273 '228' '153' 121 239 102 89 130 '169' 244 '282' 282 162 '014' 260\n",
      " '0Hi' 190 189 311 '148' '200' 231 22 '388' 86 91 138 285 238 '142' '163'\n",
      " 47 194 177 217 71 265 382 328.0 182 294 '005' '012' 222 248 7.5 296 173\n",
      " 316 216.0 49 255 '003' 199 327 253 '000' 114 299 '192' 224 200 151 400.0\n",
      " 52 393 205 36 310.0 '152' 41 410 283 235 103 117 23 196 93 324 38 374 215\n",
      " '265' '141' '020' 332 73 247 '208' '119' 180 '150' 193 '201' 132 284\n",
      " '084' '007' 135 '115' 357 383 '22' 4.5 '154' 26 187 276 169 277 42 '102'\n",
      " 304 214 289 197 '191' 146 '075' 320 191 69 32 323 266 251 39 338 241\n",
      " '224' '243' 181 '140' 168 212 '344' '166' '199' '183' 350 '202' '178' 257\n",
      " 198 450 286 '315' '311' 367 302 '144' 293 275 53 209 219 149 250 274 124\n",
      " '161' 68 309 368 335 65 380 123 '181' 280 '125' 281 72 '121' 300 '082'\n",
      " 172 66 '160' 208 291 237 '225' 150.0 '285' 179 287 342 63 218 '188' '235'\n",
      " 37 '118' 337 '067' 307 343 '182' '198' '21' '091' 314 '179' 262 339 315\n",
      " 29 366 '292' '158' '104' '109' '145' '222' '072' 230 '143' 321 '127' 229\n",
      " 108 322 '162' '151' 325 '147' '112' '138' 365 '069' 271 128 '099' '176'\n",
      " '173' '304' 303]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2932, 2933]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m val_data \u001b[38;5;241m=\u001b[39m val_data\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Evaluate performance\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(val_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], predictions)\n\u001b[0;32m     32\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(val_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(val_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2932, 2933]"
     ]
    }
   ],
   "source": [
    "# Classification and Evaluation loop\n",
    "k_values = range(1, 11)\n",
    "accuracies = []\n",
    "for k in k_values:\n",
    "    accuracies_per_split = []\n",
    "    precisions_per_split = []\n",
    "    recalls_per_split = []\n",
    "    f1_scores_per_split = []\n",
    "    roc_auc_scores_per_split = []\n",
    "    for i in range(5):\n",
    "        train_data, val_data, test_data = split_data(data)\n",
    "        # Train (no explicit training needed for KNN)\n",
    "        # Make predictions on validation set\n",
    "        predictions = [knn(train_data.iloc[:,:-1], test_point, k) for test_point in val_data.iloc[:, :-1].values]# Use .values for NumPy array\n",
    "        print(val_data.iloc[:, -1].isnull().sum())  # Check for NaNs in labels\n",
    "        print(predictions)  # Check for NaNs in predictions\n",
    "        print(len(val_data.iloc[:, -1]))\n",
    "        print(len(predictions))\n",
    "        print(val_data.iloc[:, -1].isnull().sum())\n",
    "        print(np.isnan(predictions).sum())\n",
    "        val_data.iloc[:, -1] = val_data.iloc[:, -1].fillna(val_data.iloc[:, -1].mode().iloc[0])\n",
    "        # Print unique values in the target labels column\n",
    "        print(val_data.iloc[:, -1].unique())\n",
    "        # Remove rows with non-numeric labels\n",
    "        val_data = val_data[pd.to_numeric(val_data.iloc[:, -1], errors='coerce').notnull()]\n",
    "        # Ensure the target labels are of integer type\n",
    "        val_data.iloc[:, -1] = val_data.iloc[:, -1].astype(int)\n",
    "        val_data = val_data.dropna()\n",
    "        \n",
    "        # Evaluate performance\n",
    "        accuracy = accuracy_score(val_data.iloc[:, -1], predictions)\n",
    "        precision = precision_score(val_data.iloc[:, -1], predictions, average='weighted')\n",
    "        recall = recall_score(val_data.iloc[:, -1], predictions, average='weighted')\n",
    "        f1 = f1_score(val_data.iloc[:, -1], predictions, average='weighted')\n",
    "        roc_auc = roc_auc_score(val_data.iloc[:, -1], predictions)\n",
    "        accuracies_per_split.append(accuracy)\n",
    "        precisions_per_split.append(precision)\n",
    "        recalls_per_split.append(recall)\n",
    "        f1_scores_per_split.append(f1)\n",
    "        roc_auc_scores_per_split.append(roc_auc)\n",
    "    accuracy_mean = np.mean(accuracies_per_split)\n",
    "    accuracy_std = np.std(accuracies_per_split)\n",
    "    accuracies.append((k, accuracy_mean, accuracy_std))\n",
    "    # Similarly, calculate mean and std for other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe6bd5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ks, mean_accuracies, std_accuracies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39maccuracies)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(ks, mean_accuracies)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39merrorbar(ks, mean_accuracies, yerr\u001b[38;5;241m=\u001b[39mstd_accuracies)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "ks, mean_accuracies, std_accuracies = zip(*accuracies)\n",
    "plt.plot(ks, mean_accuracies)\n",
    "plt.errorbar(ks, mean_accuracies, yerr=std_accuracies)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. k for KNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b5b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
